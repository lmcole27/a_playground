{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">Import Modules</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymupdf4llm\n",
    "import chromadb\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from openai import OpenAI\n",
    "import re\n",
    "from rapidfuzz import process, fuzz \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">Define document source directory</p>\n",
    "\n",
    "This is where I stored the municipal asset management plan pdf files that will be parsed into multiple segments of text. I will refer to these segments of text as documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdffolder = Path(\"./sourcePDF/\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">Metadata Functions</p>\n",
    "\n",
    "I created functions to add metadata to each document to help relate the document back to the municipal government structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to look up metadata from source file\n",
    "def lookupMetaData(municipality):\n",
    "    # Load the metadata CSV file\n",
    "    df = pd.read_csv('./sourceData/Municipality_MetaData.csv')\n",
    "    name_list = df['Name'].dropna().tolist()\n",
    "\n",
    "    # Find best match using fuzzy logic\n",
    "    best_match, score, index = process.extractOne(\n",
    "        municipality, name_list, scorer=fuzz.token_sort_ratio\n",
    "    )\n",
    "    if score >= 50:\n",
    "        matched_row = df[df['Name'] == best_match].iloc[0]\n",
    "        output = matched_row.to_dict()\n",
    "        return output\n",
    "    else:\n",
    "        return f\"No good match found for '{municipality}' (best guess: '{best_match}', score: {score})\"\n",
    "\n",
    "\n",
    "# Function to create metadata from lookup in source file\n",
    "def createMetaData(fname):\n",
    "    fnameSeg = re.findall(r'(\\d+)(\\_)(\\w+)', fname)\n",
    "    year = fnameSeg[0][0]\n",
    "    municipality = fnameSeg[0][2]\n",
    "    metadata = lookupMetaData(municipality)\n",
    "    docData = {\"File Name\": fname,\n",
    "                \"Name\" : metadata[\"Name\"], \n",
    "                \"year\" : year, \n",
    "                \"Municipal status\" : metadata[\"Municipal status\"], \n",
    "                \"Geographic Area\" : metadata[\"Geographic Area\"],\n",
    "                \"Upper Tier\" : metadata[\"Upper Tier\"],\n",
    "                \"website\" : metadata[\"website\"]\n",
    "                }\n",
    "    return docData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">Initialize Chromadb Client and Functions to Create Collections</p>\n",
    "\n",
    "I abstracted the creation of a chromadb collection and the addition of documents to that collection. This makes it easy to create a new collection for each embedding model and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromaClient = chromadb.PersistentClient(path=\"./chromaDB\")\n",
    "\n",
    "# Function to create a Chroma Collection \n",
    "def initializeChromaCollection(collection_name):\n",
    "    try:\n",
    "        collection = chromaClient.get_collection(name=collection_name)\n",
    "        print(f\"Collection {collection_name} found with {collection.count()} documents\")\n",
    "    except Exception as e:\n",
    "        print(f\"{e} Creating new collection...\")\n",
    "        collection = chromaClient.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        print(f\"Created collection {collection_name} with {collection.count()} documents\")\n",
    "\n",
    "# Function to add documents to a Chroma Collection        \n",
    "def addDocumentsToChromaCollection(collection_name, documents, embedding_function):\n",
    "    initializeChromaCollection(collection_name)\n",
    "    collection = chromaClient.get_collection(name=collection_name)\n",
    "    x=0\n",
    "    for doc in documents:\n",
    "        x+=1\n",
    "        text= doc.text\n",
    "        id= doc.doc_id\n",
    "        metadata = doc.metadata\n",
    "        embed_model = embedding_function(text)\n",
    "\n",
    "        collection.add(\n",
    "            documents=text,\n",
    "            embeddings=embed_model,\n",
    "            ids=id,\n",
    "            metadatas=[metadata]\n",
    "        )\n",
    "    print(f\"There were {x} documents added to {collection_name} collection.\")\n",
    "    print(f\"The number of documents in {collection_name } is {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">Define Function To Compare Expected and Recieved Response</p>\n",
    "\n",
    "The test model uses cosine similarity measure the similarity between the expected and recieved responses. It ranges from -1 opposite to 1 exactly the same, with 0 indicating no similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_similarity_score(expected, response, embedding_function):\n",
    "    expected_emb = embedding_function(expected)\n",
    "    response_emb = embedding_function(response)\n",
    "    return util.cos_sim(expected_emb, response_emb).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">OpenAI: LLM Client and API Call</p>\n",
    "\n",
    "The test model passes the context to the LLM to generate a response.  The test model evaluates the context provided from the vector database alone and also the response provided by the LLM with the assistance of the provided context.\n",
    "\n",
    "In general, the LLM scores better than the documents retrived from the vector database alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OpenAI Client\n",
    "openAIClient = OpenAI(api_key=os.environ['OPENAI_API_KEY'], organization=os.environ['ORGANIZATION'], project=os.environ['PROJECT'])\n",
    "\n",
    "def generate_response(question, context, chat_id=None, chat_mgr=None):\n",
    "    # Build base message list\n",
    "    messages = [{\"role\": \"user\", \"content\": question},\n",
    "                {\"role\": \"system\", \"content\": context}]\n",
    "    \n",
    "    # Add history if enabled\n",
    "    if chat_mgr and chat_id:\n",
    "        history = chat_mgr.get_history(chat_id)\n",
    "        messages.append({\"role\": \"system\", \"content\": str(history)})\n",
    "        chat_mgr.add_message(chat_id, \"user\", question)\n",
    "\n",
    "    # Call OpenAI\n",
    "    response = openAIClient.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "\n",
    "    # Collect response\n",
    "    full_response = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            full_response += chunk.choices[0].delta.content\n",
    "\n",
    "    # Record and summarize if needed\n",
    "    if chat_mgr and chat_id:\n",
    "        chat_mgr.add_message(chat_id, \"system\", full_response)\n",
    "        chat_mgr.summarize_history(chat_id)\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">Chat History Manager</p>\n",
    "\n",
    "The chat manager class manages chat history, passing it to the llm as context, when available. \n",
    "\n",
    "While chat history isn't shown in this version of the notebook.  I did run some tests on a smaller vector database and was surprised to find that adding chat history made the results worse! \n",
    "\n",
    "My explanation for this is that the test questions are not related and so the additional chat history context adds additional unrelated context that dillutes the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatHistoryManager:\n",
    "    def __init__(self):\n",
    "        self.histories = {}  # key = chat_id, value = list of messages\n",
    "        self.save_file = \"chat_histories.json\"\n",
    "\n",
    "    def add_message(self, chat_id, role, content):\n",
    "        if chat_id not in self.histories:\n",
    "            self.histories[chat_id] = []\n",
    "        self.histories[chat_id].append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def get_history(self, chat_id):\n",
    "        return self.histories.get(chat_id, [])\n",
    "\n",
    "    def summarize_history(self, chat_id, max_history=4):\n",
    "        history = self.get_history(chat_id)\n",
    "        if len(history) > max_history:\n",
    "            summarized_text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in history[:max_history]])\n",
    "            context = \"You are creating a succinct summary of a past conversation for reference.\"\n",
    "            summary = generate_response(question=summarized_text, context=context, chat_history=None)\n",
    "            self.histories[chat_id] = [{\"role\": \"system\", \"content\": summary}] + history[max_history:]\n",
    "\n",
    "    def save_histories(self):\n",
    "        with open(self.save_file, \"w\") as f:\n",
    "            json.dump(self.histories, f, indent=2)\n",
    "        print(\"save success\")\n",
    "\n",
    "    def load_histories(self):\n",
    "        try:\n",
    "            with open(self.save_file, \"r\") as f:\n",
    "                self.histories = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            self.histories = {}\n",
    "        print(\"load success\")\n",
    "        \n",
    "    def print_history(self, chat_id):\n",
    "        for msg in self.get_history(chat_id):\n",
    "            print(f\"{msg['role']}: {msg['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">Test Model</p>\n",
    "\n",
    "This class manages the test runs for each model.  It has the ability to print summarized results and save the full results to a file.\n",
    "\n",
    "NOTE: The variable result['documents'][0] contains the n_results which in this case is 3\n",
    "    print(len(result['documents'][0])) # =3\n",
    "    if you turn the results into a string that you can evaluate them together, \n",
    "    no need to loop through and then combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestRunner:\n",
    "    def __init__(self, collection_name, embedding_function, chat_id=None, chat_mgr=None, n_results=5):\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_function = embedding_function\n",
    "        self.chat_id=chat_id\n",
    "        self.chat_mgr = chat_mgr\n",
    "        self.test_results = []\n",
    "        self.count = 0\n",
    "        self.sum_vec = 0\n",
    "        self.sum_file = 0\n",
    "        self.sum_llm = 0\n",
    "        self.n_results = n_results\n",
    "\n",
    "    def run_test_case(self, case):\n",
    "        self.count += 1\n",
    "        query = case[\"Query\"]\n",
    "        expected = case[\"Expected Result\"]\n",
    "        source_file = case[\"Source File\"]\n",
    "\n",
    "        print(f\"Running query: {query}\")\n",
    "        collection = chromaClient.get_collection(name=self.collection_name)\n",
    "        vec = self.embedding_function(query)\n",
    "        result = collection.query(query_embeddings=[vec], n_results=self.n_results)\n",
    "\n",
    "        vec_response = str(result['documents'][0])\n",
    "        file_name = str(result['metadatas'][0][0]['File Name'])\n",
    "\n",
    "        llm_response = generate_response(query, vec_response, chat_id=self.chat_id, chat_mgr=self.chat_mgr)\n",
    "\n",
    "        vec_score = embedding_similarity_score(expected, vec_response, self.embedding_function)\n",
    "        file_score = embedding_similarity_score(source_file, file_name, self.embedding_function)\n",
    "        llm_score = embedding_similarity_score(expected, llm_response, self.embedding_function)\n",
    "\n",
    "        self.sum_vec += vec_score\n",
    "        self.sum_file += file_score\n",
    "        self.sum_llm += llm_score\n",
    "\n",
    "        self.test_results.append({\n",
    "            \"Query\": query,\n",
    "            \"Expected\": expected,\n",
    "            \"Response\": vec_response,\n",
    "            \"Embedding Similarity\": vec_score,\n",
    "            \"Source File Similarity\": file_score,\n",
    "            \"LLM Response\": llm_response,\n",
    "            \"LLM Embedding Similarity\": llm_score\n",
    "        })\n",
    "\n",
    "    def run_all(self, test_cases):\n",
    "        for case in test_cases:\n",
    "            self.run_test_case(case)\n",
    "\n",
    "    def save_results(self, fname):\n",
    "        with open(fname, \"w\", newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.test_results[0].keys())\n",
    "            writer.writeheader()\n",
    "            for r in self.test_results:\n",
    "                writer.writerow(r)\n",
    "        print(f\"Results saved to {fname}\")\n",
    "\n",
    "    def summarize(self):\n",
    "        print(f\"Total cases: {self.count}\")\n",
    "        print(f\"Avg Embedding Similarity: {self.sum_vec / self.count:.3f}\")\n",
    "        print(f\"Avg Source File Similarity: {self.sum_file / self.count:.3f}\")\n",
    "        print(f\"Avg LLM Similarity: {self.sum_llm / self.count:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">Create Llama Documents from the Source PDF Files<p>\n",
    "\n",
    "Llama documents are a convenient construct for use with vector databases.  Llama extracts text considering document structure like section headings and paragraphs keeping relevant text together.  Llama also auto generates and attaches metadata like document name, page number, and keywords. \n",
    "\n",
    "Unfortunately you still need to use a combination of pdf extraction tools if you want to extract images and tables in a more embedding friendly format.\n",
    "\n",
    "This is where I could likely most easily improve my results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported LlamaIndex\n",
      "Created 2925 documents (text segments) from the source PDFs.\n"
     ]
    }
   ],
   "source": [
    "llama_reader = pymupdf4llm.LlamaMarkdownReader()\n",
    "all_llama_docs = []\n",
    "remove_list = ['format', 'author', 'creator', 'producer', 'creationDate', 'modDate', 'trapped', 'encryption', 'file_path']\n",
    "\n",
    "for pdf in pdffolder.iterdir():\n",
    "    fname = str(pdf)\n",
    "    if fname.lower().endswith(\".pdf\"):\n",
    "        docData = createMetaData(fname)\n",
    "        \n",
    "        llama_docs = llama_reader.load_data(pdf)\n",
    "        for i, doc in enumerate(llama_docs):\n",
    "            doc.metadata = {k: v for k, v in doc.metadata.items() if k not in remove_list and v is not None}\n",
    "            for key, value in docData.items():\n",
    "                doc.metadata.update({\n",
    "                key:value\n",
    "            })   \n",
    "\n",
    "        #print(doc.metadata)\n",
    "        all_llama_docs.extend(llama_docs) \n",
    "    \n",
    "print(f\"Created {len(all_llama_docs)} documents (text segments) from the source PDFs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">Load Test Cases</p>\n",
    "\n",
    "I created a number of questions and expected responses from the Source PDF documents. The test model applies the embedding model to the questions and expected responses.  It compares the encoding of the expected response to the retrieved response using the embedding similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_case_file = \"TestCases.csv\"\n",
    "test_case_file = \"RAG_tests.csv\"\n",
    "test_cases = []\n",
    "\n",
    "\n",
    "with open(test_case_file, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        test_cases.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5;\">Model Comparison</p>\n",
    "\n",
    "I compared retrievals using four different embedding models:\n",
    "<ol style=\"line-height: 1.5;\">\n",
    "<li>Sentence Transformer all-MiniLM-L6-v2 Embedding Model</li>\n",
    "<li>HuggingFace Sentence Transformers all-mpnet-base-v2</li>\n",
    "<li>OpenAI text-embedding-3-small</li>\n",
    "<li>OpenAI text-embedding-3-large\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "For each test case, I followed a 3-step process:\n",
    "<ol style=\"line-height: 1.5;\">\n",
    "<li style=\"font-weight: bold; color: #555;\">Load the Embedding Model and Set the Embedding Function:</li> \n",
    "    <ul><li>For the best results the generally accepted practice is to use the same model to embed the document segments in the vector database, the question, the expected results, the retrieved context and the llm response.</li>\n",
    "    <li>The model needs to be loaded in advance to avoid reloading every time you call the embedding function.</li>\n",
    "    <li>Models have varied function calls for creating an embedding.</li>\n",
    "</ul>\n",
    "<li style=\"font-weight: bold; color: #555;\">Create Chroma Database Collection</li>\n",
    "    <ul><li>I created a chroma collection for each embedding model in the same database.  This enables you to easily query the databases and compare the embedding models.</li></ul>\n",
    "<li style=\"font-weight: bold; color: #555;\">Run Test Model</li>\n",
    "    <ul><li>I then use the test cases to evaluate and compare the performance of the embedding models.</li></ul>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">TEST CASE 1: Sentence Transformer all-MiniLM-L6-v2 Embedding Model</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:14px; color:#3F51B5\">1: Load Embedding Model and Set Function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents in all-MiniLM-L6-v2: 0\n"
     ]
    }
   ],
   "source": [
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embedding_function(text):\n",
    "    return embed_model.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:14px; color:#3F51B5\">2: Create Collection</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection all-MiniLM-L6-v2 found with 0 documents\n",
      "There were 2925 documents added to all-MiniLM-L6-v2 collection.\n",
      "The number of documents in all-MiniLM-L6-v2: 2925\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"all-MiniLM-L6-v2\"\n",
    "documents = all_llama_docs\n",
    "addDocumentsToChromaCollection(collection_name, documents, embedding_function)\n",
    "\n",
    "collection_name = \"all-MiniLM-L6-v2\"\n",
    "collection = chromaClient.get_collection(name=collection_name)\n",
    "print(f\"The number of documents in {collection_name }: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:14px; color:#3F51B5\">3: Run Test Model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query: What is the estimated total value of the City of Waterloo's infrastructure?\n",
      "Running query: Which regulation does the City of Waterloo's Asset Management Plan comply with?\n",
      "Running query: Who contributed to the development of the Asset Management Plan?\n",
      "Running query: What is the forecasted decline in performance of tax-base funded assets over 25 years?\n",
      "Running query: What is the primary funding source mentioned for infrastructure renewal?\n",
      "Running query: What is the Waterloo Decision Support System (DSS)?\n",
      "Running query: What is the city's current target for overall road PQI?\n",
      "Running query: What is the estimated cost of replacing the city's transportation assets?\n",
      "Running query: What funding increase was approved by the council for 2024-2026?\n",
      "Running query: What are some of the key asset classes identified in the plan?\n",
      "Total cases: 10\n",
      "Avg Embedding Similarity: 0.290\n",
      "Avg Source File Similarity: 0.719\n",
      "Avg LLM Similarity: 0.483\n",
      "Results saved to all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "st_runner = TestRunner(collection_name=collection_name, embedding_function=embedding_function)\n",
    "st_runner.run_all(test_cases)\n",
    "st_runner.summarize()\n",
    "st_runner.save_results(fname=\"results/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">TEST CASE 2: HuggingFace Sentence Transformers all-mpnet-base-v2</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">1: Load Embedding Model and Set Function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "def embedding_function_mpnet(text):\n",
    "    return embed_model.get_text_embedding(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">2: Create Collection</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection all-mpnet-base-v2 found with 0 documents\n",
      "There were 2925 documents added to all-mpnet-base-v2 collection.\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"all-mpnet-base-v2\"\n",
    "documents = all_llama_docs\n",
    "addDocumentsToChromaCollection(collection_name, documents, embedding_function_mpnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents in all-mpnet-base-v2: 2925\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"all-mpnet-base-v2\"\n",
    "collection = chromaClient.get_collection(name=collection_name)\n",
    "print(f\"The number of documents in {collection_name }: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">3: Run Test Model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query: What is the estimated total value of the City of Waterloo's infrastructure?\n",
      "Running query: Which regulation does the City of Waterloo's Asset Management Plan comply with?\n",
      "Running query: Who contributed to the development of the Asset Management Plan?\n",
      "Running query: What is the forecasted decline in performance of tax-base funded assets over 25 years?\n",
      "Running query: What is the primary funding source mentioned for infrastructure renewal?\n",
      "Running query: What is the Waterloo Decision Support System (DSS)?\n",
      "Running query: What is the city's current target for overall road PQI?\n",
      "Running query: What is the estimated cost of replacing the city's transportation assets?\n",
      "Running query: What funding increase was approved by the council for 2024-2026?\n",
      "Running query: What are some of the key asset classes identified in the plan?\n",
      "Total cases: 10\n",
      "Avg Embedding Similarity: 0.480\n",
      "Avg Source File Similarity: 0.745\n",
      "Avg LLM Similarity: 0.553\n",
      "Results saved to results/all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"all-mpnet-base-v2\"\n",
    "\n",
    "mpnet_runner = TestRunner(collection_name=collection_name, embedding_function=embedding_function_mpnet)\n",
    "mpnet_runner.run_all(test_cases)\n",
    "mpnet_runner.summarize()\n",
    "st_runner.save_results(fname=\"results/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">TEST CASE 3: OpenAI text-embedding-3-small</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">1: Set Embedding Model and Function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"text-embedding-3-small\"\n",
    "\n",
    "def embedding_function_openai_sm(text):\n",
    "    embedding = openAIClient.embeddings.create(input = text, model=embed_model)\n",
    "    return embedding.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">2: Create Collection</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection openai-text-embedding-3-small found with 0 documents\n",
      "There were 2925 documents added to openai-text-embedding-3-small collection.\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"openai-text-embedding-3-small\"\n",
    "documents = all_llama_docs\n",
    "addDocumentsToChromaCollection(collection_name, documents, embedding_function_openai_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">3: Run Test Model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query: What is the estimated total value of the City of Waterloo's infrastructure?\n",
      "Running query: Which regulation does the City of Waterloo's Asset Management Plan comply with?\n",
      "Running query: Who contributed to the development of the Asset Management Plan?\n",
      "Running query: What is the forecasted decline in performance of tax-base funded assets over 25 years?\n",
      "Running query: What is the primary funding source mentioned for infrastructure renewal?\n",
      "Running query: What is the Waterloo Decision Support System (DSS)?\n",
      "Running query: What is the city's current target for overall road PQI?\n",
      "Running query: What is the estimated cost of replacing the city's transportation assets?\n",
      "Running query: What funding increase was approved by the council for 2024-2026?\n",
      "Running query: What are some of the key asset classes identified in the plan?\n",
      "Total cases: 10\n",
      "Avg Embedding Similarity: 0.428\n",
      "Avg Source File Similarity: 0.758\n",
      "Avg LLM Similarity: 0.530\n",
      "Results saved to results/openai-text-embedding-3-small\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"openai-text-embedding-3-small\"\n",
    "\n",
    "openai_sm_runner = TestRunner(collection_name=collection_name, embedding_function=embedding_function_openai_sm)\n",
    "openai_sm_runner.run_all(test_cases)\n",
    "openai_sm_runner.summarize()\n",
    "openai_sm_runner.save_results(fname=\"results/openai-text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">TEST CASE 4: OpenAI text-embedding-3-large</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">1: Set Embedding Model and Function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"text-embedding-3-large\"\n",
    "\n",
    "def embedding_function_openai_lg(text):\n",
    "    embedding = openAIClient.embeddings.create(input = text, model=embed_model)\n",
    "    return embedding.data[0].embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">2: Create Collection</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection [openai-text-embedding-3-large] does not exists Creating new collection...\n",
      "Created collection openai-text-embedding-3-large with 0 documents\n",
      "There were 2925 documents added to openai-text-embedding-3-large collection.\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"openai-text-embedding-3-large\"\n",
    "documents = all_llama_docs\n",
    "addDocumentsToChromaCollection(collection_name, documents, embedding_function_openai_lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px; color:#3F51B5\">3: Run Test Model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query: What is the estimated total value of the City of Waterloo's infrastructure?\n",
      "Running query: Which regulation does the City of Waterloo's Asset Management Plan comply with?\n",
      "Running query: Who contributed to the development of the Asset Management Plan?\n",
      "Running query: What is the forecasted decline in performance of tax-base funded assets over 25 years?\n",
      "Running query: What is the primary funding source mentioned for infrastructure renewal?\n",
      "Running query: What is the Waterloo Decision Support System (DSS)?\n",
      "Running query: What is the city's current target for overall road PQI?\n",
      "Running query: What is the estimated cost of replacing the city's transportation assets?\n",
      "Running query: What funding increase was approved by the council for 2024-2026?\n",
      "Running query: What are some of the key asset classes identified in the plan?\n",
      "Total cases: 10\n",
      "Avg Embedding Similarity: 0.432\n",
      "Avg Source File Similarity: 0.769\n",
      "Avg LLM Similarity: 0.524\n",
      "Results saved to results/openai-text-embedding-3-large\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"openai-text-embedding-3-large\"\n",
    "\n",
    "openai_lg_runner = TestRunner(collection_name=collection_name, embedding_function=embedding_function_openai_sm)\n",
    "openai_lg_runner.run_all(test_cases)\n",
    "openai_lg_runner.summarize()\n",
    "openai_lg_runner.save_results(fname=\"results/openai-text-embedding-3-large\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
